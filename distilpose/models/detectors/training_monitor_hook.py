# Copyright (c) OpenMMLab. All rights reserved.
import numpy as np
import torch
from mmcv.runner import HOOKS, Hook
import warnings


@HOOKS.register_module()
class TrainingMonitorHook(Hook):
    """í•™ìŠµ ì¤‘ ì´ìƒ ì§•í›„ë¥¼ ê°ì§€í•˜ëŠ” Hook
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. Loss ê°’ì˜ ê¸‰ê²©í•œ ë³€í™” ê°ì§€
    2. Heatmap ì¶œë ¥ì˜ ì´ìƒ ê°ì§€ (NaN, Inf, ê·¹ë‹¨ê°’)
    3. Tokenì˜ í†µê³„ ì¶”ì 
    4. ì„±ëŠ¥ í•˜ë½ ê°ì§€
    
    Args:
        check_interval (int): ì²´í¬ ì£¼ê¸° (iteration). Default: 50
        loss_spike_threshold (float): Loss ê¸‰ì¦ ê°ì§€ ìž„ê³„ê°’. Default: 2.0
        performance_drop_threshold (float): ì„±ëŠ¥ í•˜ë½ ê°ì§€ ìž„ê³„ê°’. Default: 0.05
        save_stats (bool): í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ìž¥í• ì§€ ì—¬ë¶€. Default: True
    """
    
    def __init__(self, 
                 check_interval=50,
                 loss_spike_threshold=2.0,
                 performance_drop_threshold=0.05,
                 save_stats=True):
        self.check_interval = check_interval
        self.loss_spike_threshold = loss_spike_threshold
        self.performance_drop_threshold = performance_drop_threshold
        self.save_stats = save_stats
        
        # í†µê³„ ì €ìž¥
        self.loss_history = []
        self.heatmap_stats_history = []
        self.token_stats_history = []
        self.best_ap = 0.0
        
    def before_run(self, runner):
        """í•™ìŠµ ì‹œìž‘ ì „ ì´ˆê¸°í™”"""
        runner.logger.info("=" * 80)
        runner.logger.info("ðŸ” TrainingMonitorHook activated")
        runner.logger.info(f"  - Check interval: {self.check_interval} iterations")
        runner.logger.info(f"  - Loss spike threshold: {self.loss_spike_threshold}x")
        runner.logger.info(f"  - Performance drop threshold: {self.performance_drop_threshold}")
        runner.logger.info("=" * 80)
        
    def after_train_iter(self, runner):
        """ê° iteration í›„ ì²´í¬"""
        if not self.every_n_iters(runner, self.check_interval):
            return
            
        # Loss ì²´í¬
        if 'loss' in runner.log_buffer.output:
            current_loss = runner.log_buffer.output['loss']
            self._check_loss(runner, current_loss)
            
    def after_train_epoch(self, runner):
        """Epoch ì¢…ë£Œ í›„ ì²´í¬"""
        epoch = runner.epoch + 1
        runner.logger.info("")
        runner.logger.info("=" * 80)
        runner.logger.info(f"ðŸ“Š Epoch {epoch} Training Summary")
        runner.logger.info("=" * 80)
        
        # Loss summary
        if len(self.loss_history) > 0:
            recent_losses = self.loss_history[-10:]
            avg_loss = np.mean(recent_losses)
            std_loss = np.std(recent_losses)
            runner.logger.info(f"  Loss - Recent Avg: {avg_loss:.4f} Â± {std_loss:.4f}")
            runner.logger.info(f"       - Min: {min(recent_losses):.4f}, Max: {max(recent_losses):.4f}")
            
        runner.logger.info("=" * 80)
        runner.logger.info("")
        
    def after_val_epoch(self, runner):
        """Validation í›„ ì„±ëŠ¥ ì²´í¬"""
        if hasattr(runner, 'eval_res') and runner.eval_res is not None:
            if 'AP' in runner.eval_res:
                current_ap = runner.eval_res['AP']
                self._check_performance(runner, current_ap)
                
    def _check_loss(self, runner, current_loss):
        """Loss ì´ìƒ ê°ì§€"""
        # NaN/Inf ì²´í¬
        if np.isnan(current_loss) or np.isinf(current_loss):
            runner.logger.warning("âš ï¸  WARNING: Loss is NaN or Inf!")
            runner.logger.warning(f"   Iteration: {runner.iter}")
            runner.logger.warning(f"   Loss value: {current_loss}")
            return
            
        # Loss ê¸‰ì¦ ì²´í¬
        if len(self.loss_history) > 5:
            recent_avg = np.mean(self.loss_history[-5:])
            if current_loss > recent_avg * self.loss_spike_threshold:
                runner.logger.warning("âš ï¸  WARNING: Loss spike detected!")
                runner.logger.warning(f"   Current: {current_loss:.4f}")
                runner.logger.warning(f"   Recent avg: {recent_avg:.4f}")
                runner.logger.warning(f"   Ratio: {current_loss/recent_avg:.2f}x")
                
        self.loss_history.append(current_loss)
        
        # ë„ˆë¬´ ë§Žì´ ìŒ“ì´ë©´ ì˜¤ëž˜ëœ ê²ƒ ì œê±°
        if len(self.loss_history) > 1000:
            self.loss_history = self.loss_history[-1000:]
            
    def _check_performance(self, runner, current_ap):
        """ì„±ëŠ¥ í•˜ë½ ì²´í¬"""
        if current_ap > self.best_ap:
            improvement = current_ap - self.best_ap
            runner.logger.info(f"ðŸŽ‰ New Best AP: {current_ap:.4f} (+{improvement:.4f})")
            self.best_ap = current_ap
        elif self.best_ap > 0 and (self.best_ap - current_ap) > self.performance_drop_threshold:
            drop = self.best_ap - current_ap
            runner.logger.warning("âš ï¸  WARNING: Performance drop detected!")
            runner.logger.warning(f"   Current AP: {current_ap:.4f}")
            runner.logger.warning(f"   Best AP: {self.best_ap:.4f}")
            runner.logger.warning(f"   Drop: {drop:.4f}")
            
    def after_run(self, runner):
        """í•™ìŠµ ì¢…ë£Œ í›„ ìµœì¢… í†µê³„"""
        if not self.save_stats:
            return
            
        import os
        stats_file = os.path.join(runner.work_dir, 'training_stats.txt')
        
        with open(stats_file, 'w') as f:
            f.write("Training Statistics\n")
            f.write("=" * 80 + "\n\n")
            
            if len(self.loss_history) > 0:
                f.write("Loss Statistics:\n")
                f.write(f"  - Mean: {np.mean(self.loss_history):.4f}\n")
                f.write(f"  - Std: {np.std(self.loss_history):.4f}\n")
                f.write(f"  - Min: {min(self.loss_history):.4f}\n")
                f.write(f"  - Max: {max(self.loss_history):.4f}\n")
                f.write(f"  - Final: {self.loss_history[-1]:.4f}\n\n")
                
            f.write(f"Best AP: {self.best_ap:.4f}\n")
            
        runner.logger.info(f"ðŸ“ Training statistics saved to: {stats_file}")


@HOOKS.register_module()
class DetailedLossLogHook(Hook):
    """ê° Loss í•­ëª©ì„ ìžì„¸ížˆ ë¡œê¹…í•˜ëŠ” Hook
    
    Args:
        log_interval (int): ë¡œê¹… ì£¼ê¸°. Default: 10
    """
    
    def __init__(self, log_interval=10):
        self.log_interval = log_interval
        
    def after_train_iter(self, runner):
        """ê° iteration í›„ ìƒì„¸ loss ë¡œê¹…"""
        if not self.every_n_iters(runner, self.log_interval):
            return
            
        # Log bufferì—ì„œ ëª¨ë“  loss ê°€ì ¸ì˜¤ê¸°
        log_items = {}
        for key, val in runner.log_buffer.output.items():
            if 'loss' in key.lower():
                log_items[key] = val
                
        if len(log_items) > 0:
            log_str = f"Iter [{runner.iter}] "
            for key, val in log_items.items():
                if isinstance(val, (int, float)):
                    log_str += f"{key}: {val:.6f}, "
            runner.logger.info(log_str.rstrip(', '))

